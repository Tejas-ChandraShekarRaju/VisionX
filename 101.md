##Cost Functions - 
The goal of using a cost function is typically to adjust the inputs (parameters, configurations) to minimize the output (cost/error) for 
better model performance or accurate representation of data. This doesnâ€™t mean manually adjusting the input parameters. Has to be done algorithmically. 
This means that the 'cost' of how well a dataset is clustered is measured by how spread out the farthest points are within the clusters. 
If clusters have smaller radii, it indicates that the points are closer to their respective centers,making the clustering more compact and potentially better 
in terms of this cost function.


##Something about Hierarchical Clustering
In other words, it's questioning whether, no matter how many clusters you want to divide your data into, 
there's a method within hierarchical clustering that comes close to the best possible way of dividing them based on some fair measure of quality.

##Largest Radius of a Cluster
Imagine you've divided your data into clusters. Each cluster has its own center, and the radius of a cluster is the distance from that center to the farthest point within 
that cluster. So, the largest radius among all these clusters gives you the cost of that specific clustering.

Theorem 1 Take the cost of a clustering to be the largest radius of its clus-
ters. Then, any data set in any metric space has a hierarchical clustering in
which, for each k, the induced k-clustering has cost at most eight times that
of the optimal k-clustering. - explanation below
The statement is asserting that in this hierarchical clustering method, for any given 'k', the clustering it produces will have a cost (largest radius among clusters) that is at most eight times worse than the cost of the best possible clustering for that same 'k'. This comparison indicates that while the induced clustering might not be the absolute best for each 'k', it won't be significantly worse than the best possible clustering arrangement for that specific number of clusters.

    Approximation Ratios: Refers to how close an algorithm's solution is to the optimal (best possible) solution for a given problem. A good algorithm often has a bounded approximation ratio, meaning it consistently provides solutions that are reasonably close to the best solution.

    Unbounded Approximation Ratios: Indicates that in certain situations or cases, these heuristics don't perform well. Their solutions can deviate significantly from the optimal solution, and this deviation can become increasingly large or unbounded as the problem size or complexity grows.

Problem: Why neural network for digit recognition needs 10 outputs (neurons) instead of 4.

Answer: 10 output neurons each learn specific features ("historical shapes") of a digit, while 4 outputs struggle to encode complex relations between bits and features.
